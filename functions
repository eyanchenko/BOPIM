## Functions to implement BOPIM algorithm in Python

# BOPIM algorithm for influence maximization
# E: edge list
# kk: seed set size
# lam: infection parameter
# B: budget
# num_samps: number of initial evaluations
# obj_sims: number of MC evaluations for influence spread function
# prior: shrinkage prior ["Horseshoe", "DL", "R2D2"]
# mc_burn: burn-in samples for Gibbs sampler
# mc_iters: MCMC samples for Gibbs sampler
# mc_cores: number of cores for objective function evaluation

def bayesopt(E, kk=1, lam=0.01, B=5, num_samps=20, obj_sims=100, prior="Horseshoe", mc_burn=1000, mc_iters=5000, mc_cores=7):
    
    # Sample points to evaluate objective function at.
    N = num_samps
    nsims = obj_sims
    
    # Compute degrees and sample proportionally
    G = nx.from_pandas_edgelist(E, 'v1', 'v2')
    n = len(G.nodes) # number of nodes
    x = np.zeros((N,n))
    y = np.zeros(N)
    
    degs = np.array(list(dict(G.degree()).values()))
    degs = degs / sum(degs)
    
    S_samp = np.zeros((N,kk))
    for i in range(N):
        samp = list(np.random.choice(a=n, size=kk, replace=False, p=degs))
        S_samp[i,:] = samp
        x[i, samp] = 1
        
        
        
    out = [sigma_mc(E, list(S_samp[i,:]), lam, nsims, mc_cores) for i in range(N)]    
        
    #pool = get_context("fork").Pool(7)
    #out = pool.starmap(sigma_mc_map, zip(repeat(E), S_samp, repeat(lam), repeat(nsims), repeat(False)) )
    #pool.close()
    
    y = np.asarray(out)
    meany = statistics.mean(y)
    y = y - meany

    # while still have iterations remaining
    for b in range(B):
            
        # First run MCMC code
        if prior=="Horseshoe":
            alpha = horseshoe(y=y, X=x, mc_burn=mc_burn, mc_iters=mc_iters)
        elif prior=="R2D2":
            alpha = r2d2(y=y, X=x, mc_burn=mc_burn, mc_iters=mc_iters)
        elif prior=="DL":
            alpha = dirlap(y=y, X=x, mc_burn=mc_burn, mc_iters=mc_iters)
            
        # Then select largest parameters as seed nodes
        S_opt = list(np.argpartition(np.median(alpha, axis=0), -kk)[-kk:])
        x_opt = np.zeros(n)
        x_opt[S_opt] = 1
        
        y = np.append(y, sigma_mc(E, S_opt, lam, nsims, mc_cores) - meany) 
        x = np.append(x, [x_opt], axis=0)
        print(b)
    
    
    # Select largest value
    y_opt = max(y) + meany
    S_opt = list(np.where(x[np.argmax(y),:]>0)[0])
    
    return S_opt, y_opt




# Single simulation of the SI process
# E:     input graph (as edge list)
# S:     seed nodes
# lam:   susceptibility parameter

def sigma(E, S, lam, xxx=0):
    # I will keep track of infected nodes
    I = list(S.copy())
    
    # Unique time snapshots of the network
    T = pd.unique(E["t"])
    
    for t in T:
    
        # Create graph
        G = nx.from_pandas_edgelist(E[E["t"]==t], 'v1', 'v2')
        new_infect = []
        # Loop over infected nodes
        for node in I:
            np.random.seed()
            if node in G.nodes:
                success = np.random.uniform(0,1,len(list(G.neighbors(node)))) < lam
                new_infect += list(np.extract(success, list(G.neighbors(node))))
        
        I.extend(new_infect)
        I = list(set(I))
    
    return(len(I))  

# Multiple simulations of the SI process
# E:     input graph (as edge list)
# S:     seed nodes
# lam:   susceptibility parameter
# nsims: number of simulations
# mc_cores: number of cores available for parallel evaluation

def sigma_mc(E, S, lam, nsims=100, mc_cores=7):

    # Repeat function nsims times
    pool = get_context("fork").Pool(mc_cores)
    out = pool.starmap(sigma, zip(repeat(E), repeat(S), repeat(lam), range(nsims)))
    pool.close()
                       
    out = list(out)
    
    return(statistics.mean(out))



# Horseshoe Gibbs sampler
# y: response vector
# X: covariate matrix
# mc_burn: number of burn-in samples
# mc_iters: number of MCMC samples

def horseshoe(y, X, mc_burn=1000, mc_iters=5000):
    N = X.shape[0]
    n = X.shape[1]
    
   # Initialize MCMC
    CHOL = np.linalg.cholesky(np.transpose(X) @ X + np.identity(n))
    alpha = CHOL @ np.random.normal(0, 1, n)
    sigma2 = 1 / np.random.gamma(1, 1/1)
    beta2 = np.zeros(n)
    for i in range(n):   
        beta2[i] = 1 / np.random.gamma(1, 1/1)
    
    tau2 = 1 / np.random.gamma(1, 1/1)
    nu = np.zeros(n)
    for i in range(n):   
        nu[i] = 1 / np.random.gamma(1, 1/1) 
    xi = 1 / np.random.gamma(1, 1/1)  
    
    alpha_post = np.zeros((mc_iters,n))
    # GO! 
    for i in range(mc_burn+mc_iters):
        # Sample from alpha
        
        #A = np.transpose(X) @ X + np.diag(1/(tau2*beta2))
        #M = scipy.sparse.csr_matrix(A)
        #Ainv = scipy.linalg.inv(A)
        #Ainv = Ainv.toarray()
        Sinv = np.diag(1/(tau2*beta2))
        
        L = np.linalg.cholesky(np.transpose(X) @ X + Sinv)
        Linv = np.linalg.inv(L)
        alpha = np.transpose(Linv) @ Linv @ np.transpose(X) @ y + math.sqrt(sigma2)*Linv @ np.random.normal(0, 1, n)

        # Sample from sigma2
        sigma2 = 1/np.random.gamma((N+n)/2, 1/(sum((y-X@alpha)**2)/2 + sum(alpha**2/(2*tau2*beta2))))

        # Sample from beta2 and nu
        for k in range(n):
            beta2[k] = 1/np.random.gamma(1, 1/(1/nu[k] + alpha[k]**2/(2*tau2*sigma2)))
            nu[k] = 1/np.random.gamma(1, 1/(1+1/beta2[k]))

        # Sample from tau2
        tau2 = 1/np.random.gamma((n+1)/2, 1/(1/xi + 1/(2*sigma2)*sum(alpha**2/beta2)))

        # Sample from xi
        xi = 1/np.random.gamma(1, 1/(1+1/tau2))
                
        if i >= mc_burn:
            alpha_post[i-mc_burn, :] = alpha
        
    return alpha_post

# Dirichlet-Laplace Gibbs sampler
# y: response vector
# X: covariate matrix
# mc_burn: number of burn-in samples
# mc_iters: number of MCMC samples

def dirlap(y, X, mc_burn=1000, mc_iters=5000):
    N = X.shape[0]
    p = X.shape[1]
    
    b = 0.5
    a = 0.5
    
   # Initialize MCMC
    psi = np.random.gamma(1,1, size=p)
    phi = np.random.exponential(1, size=p)
    phi = phi/sum(phi)
    sigma2 = np.random.gamma(1,1, size=1)
    tau  = np.random.gamma(1,1, size=1)
    

    Sinv = np.diag(1/(psi*(phi**2)*tau**2))
    L = np.linalg.cholesky(np.transpose(X) @ X + Sinv)
    Linv = np.linalg.inv(L)
    alpha = np.transpose(Linv) @ Linv @ np.transpose(X) @ y + math.sqrt(sigma2)*Linv @ np.random.normal(0, 1, p)

    
    alpha_post = np.zeros((mc_iters,p))
    # GO! 
    for i in range(mc_burn+mc_iters):
        # Sample from alpha
        L = np.linalg.cholesky(np.transpose(X) @ X + Sinv)
        Linv = np.linalg.inv(L)
        alpha = np.transpose(Linv) @ Linv @ np.transpose(X) @ y + math.sqrt(sigma2)*Linv @ np.random.normal(0, 1, p)
    
        # Sample from sigma2
        sigma2 = 1/np.random.gamma(shape=((0.001+ N+p)/2), scale=(0.001 + 0.5*sum((y-X@alpha)**2) + 0.5*sum(alpha**2/(tau**2*psi*(phi**2)))))
        #sigma2 = 1
        
        # Sample from psi
        for j in range(p):
            psi[j] = 1/scipy.stats.invgauss.rvs(mu=math.sqrt(sigma2)*phi[j]*tau/abs(alpha[j]))
        
        #psi = np.asarray([1, 1, 1, 1, 1])
        
        # Sample from w
        tau = gigrnd(p*a-p,1,2*sum(abs(alpha)/(math.sqrt(sigma2)*phi)))
        #tau = 10    
            
        T = np.zeros(p) 
        
        # Sample from phi
        for j in range(p):
            T[j] = gigrnd(a-1, 1, 2*abs(alpha[j])/math.sqrt(sigma2))
        
        phi[j] = T[j]/sum(T)
        #phi = np.asarray([0.2, 0.2, 0.2, 0.2, 0.2])
        
        # Update Sinv
        Sinv = np.diag(1/(psi*(phi**2)*tau**2))
        
        if i >= mc_burn:
            alpha_post[i-mc_burn, :] = alpha
        
    return alpha_post


# R2D2 Gibbs sampler
# y: response vector
# X: covariate matrix
# mc_burn: number of burn-in samples
# mc_iters: number of MCMC samples


def r2d2(y, X, mc_burn=1000, mc_iters=5000):
    N = X.shape[0]
    n = X.shape[1]
    
    b = 0.5
    api = 1/(n**(b/2) * N**(b/2) * math.log(N))
    a = n * api
    
   # Initialize MCMC
    psi = np.random.gamma(1,1, size=n)
    phi = np.random.exponential(1, size=n)
    phi = phi/sum(phi)
    sigma2 = np.random.gamma(1,1, size=1)
    w  = np.random.gamma(1,1, size=1)
    xi = np.random.gamma(1,1)
    

    Sinv = np.diag(2/(w*psi*phi))
    L = np.linalg.cholesky(np.transpose(X) @ X + Sinv)
    Linv = np.linalg.inv(L)
    alpha = np.transpose(Linv) @ Linv @ np.transpose(X) @ y + math.sqrt(sigma2)*Linv @ np.random.normal(0, 1, n)

    alpha_post = np.zeros((mc_iters,n))
    # GO! 
    for i in range(mc_burn+mc_iters):
        # Sample from alpha
        L = np.linalg.cholesky(np.transpose(X) @ X + Sinv)
        Linv = np.linalg.inv(L)
        alpha = np.transpose(Linv) @ Linv @ np.transpose(X) @ y + math.sqrt(sigma2)*Linv @ np.random.normal(0, 1, n)

        # Sample from sigma2
        sigma2 = 1/np.random.gamma((0.001+ N+n)/2, (0.001 + 0.5*sum((y-X@alpha)**2) + 0.5*sum(alpha**2/(w*psi*phi/2))))

        # Sample from psi
        for j in range(n):
            psi[j] = 1/scipy.stats.invgauss.rvs(mu=math.sqrt(sigma2*phi[j]*w/2)/abs(alpha[j]))
        
        # Sample from w
        w = gigrnd(a-n/2,2*xi,2*sum(alpha**2/(sigma2*phi*psi)))
            
        # Sample xi
        xi = np.random.gamma(a+b, 1+ w, size=1)
        
        # Sample from phi
        for j in range(n):
            phi[j] = gigrnd(api-0.5, 2*xi, 2*alpha[j]**2/(sigma2*psi[j]))
        
        phi = phi/sum(phi)
        
        # Update Sinv
        Sinv = np.diag(2/(w*psi*phi))
        
        if i >= mc_burn:
            alpha_post[i-mc_burn, :] = alpha
        
    return alpha_post


# Generalized inverse gaussian sampler
# From: https://github.com/getian107/PRScsx

"""
Random variate generator for the generalized inverse Gaussian distribution.
Reference: L Devroye. Random variate generation for the generalized inverse Gaussian distribution.
           Statistics and Computing, 24(2):239-246, 2014.
"""

import math
from numpy import random


def psix(x, alpha, lam):
    f = -alpha*(math.cosh(x)-1.0)-lam*(math.exp(x)-x-1.0)
    return f


def dpsi(x, alpha, lam):
    f = -alpha*math.sinh(x)-lam*(math.exp(x)-1.0)
    return f


def g(x, sd, td, f1, f2):
    if (x >= -sd) and (x <= td):
        f = 1.0
    elif x > td:
        f = f1
    elif x < -sd:
        f = f2

    return f


def gigrnd(p, a, b):
    # setup -- sample from the two-parameter version gig(lam,omega)
    p = float(p); a = float(a); b = float(b)
    lam = p
    omega = math.sqrt(a*b)

    if lam < 0:
        lam = -lam
        swap = True
    else:
        swap = False

    alpha = math.sqrt(math.pow(omega,2)+math.pow(lam,2))-lam

    # find t
    x = -psix(1.0, alpha, lam)
    if (x >= 0.5) and (x <= 2.0):
        t = 1.0
    elif x > 2.0:
        if (alpha == 0) and (lam == 0):
            t = 1.0
        else:
            t = math.sqrt(2.0/(alpha+lam))
    elif x < 0.5:
        if (alpha == 0) and (lam == 0):
            t = 1.0
        else:
            t = math.log(4.0/(alpha+2.0*lam))

    # find s
    x = -psix(-1.0, alpha, lam)
    if (x >= 0.5) and (x <= 2.0):
        s = 1.0
    elif x > 2.0:
        if (alpha == 0) and (lam == 0):
            s = 1.0
        else:
            s = math.sqrt(4.0/(alpha*math.cosh(1)+lam))
    elif x < 0.5:
        if (alpha == 0) and (lam == 0):
            s = 1.0
        elif alpha == 0:
            s = 1.0/lam
        elif lam == 0:
            s = math.log(1.0+1.0/alpha+math.sqrt(1.0/math.pow(alpha,2)+2.0/alpha))
        else:
            s = min(1.0/lam, math.log(1.0+1.0/alpha+math.sqrt(1.0/math.pow(alpha,2)+2.0/alpha)))

    # find auxiliary parameters
    eta = -psix(t, alpha, lam)
    zeta = -dpsi(t, alpha, lam)
    theta = -psix(-s, alpha, lam)
    xi = dpsi(-s, alpha, lam)

    p = 1.0/xi
    r = 1.0/zeta

    td = t-r*eta
    sd = s-p*theta
    q = td+sd

    # random variate generation
    while True:
        U = random.random()
        V = random.random()
        W = random.random()
        if U < q/(p+q+r):
            rnd = -sd+q*V
        elif U < (q+r)/(p+q+r):
            rnd = td-r*math.log(V)
        else:
            rnd = -sd+p*math.log(V)

        f1 = math.exp(-eta-zeta*(rnd-t))
        f2 = math.exp(-theta+xi*(rnd+s))
        if W*g(rnd, sd, td, f1, f2) <= math.exp(psix(rnd, alpha, lam)):
            break

    # transform back to the three-parameter version gig(p,a,b)
    rnd = math.exp(rnd)*(lam/omega+math.sqrt(1.0+math.pow(lam,2)/math.pow(omega,2)))
    if swap:
        rnd = 1.0/rnd

    rnd = rnd/math.sqrt(a/b)
    return rnd

    
