# Load libraries
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import random
import math
import statistics
import time
import warnings
warnings.filterwarnings('ignore')

from scipy.optimize import minimize
from multiprocessing import Pool
import networkx as nx
from multiprocessing import get_context
from itertools import repeat
from itertools import chain
from multiprocessing import Pool
from multiprocessing import get_context
from igraph import Graph
from scipy.stats import norm
import scipy

try:
    from tqdm import tqdm
except ImportError:
    def tqdm(iterable):
        return iterable


# Function to pre-process (bin) times
def pre_process_times(E, grps = 10):
    out = E.copy()
    if type(grps)!=int:
        out["t"] = pd.cut(out["t"], bins=grps, labels=list(range(len(grps)-1)))
    else:
        out["t"] = pd.cut(out["t"], bins=grps, labels=list(range(grps)))

    out = out.drop_duplicates()
    out = out.reset_index(drop=True)

    # Convert node numbers from 0 to n
    G = nx.from_pandas_edgelist(E, 'v1', 'v2') 
    orig_nodes = list(G.nodes())
    node_dict = {orig_nodes[i]: i for i in range(len(orig_nodes))}
    out = pd.concat([out[["v1","v2"]].replace(to_replace=node_dict), out["t"]], axis=1)
    return out


# Function to simulate SI process (new idea)
# E:     input graph (as edge list)
# S:     seed nodes
# lam:   susceptibility parameter
def sigma(E, S, lam, xxx=0):
    # I will keep track of infected nodes
    I = list(S.copy())
    
    # Unique time snapshots of the network
    T = pd.unique(E["t"])
    
    for t in T:
    
        # Create graph
        G = nx.from_pandas_edgelist(E[E["t"]==t], 'v1', 'v2')
        new_infect = []
        # Loop over infected nodes
        for node in I:
            np.random.seed()
            if node in G.nodes:
                success = np.random.uniform(0,1,len(list(G.neighbors(node)))) < lam
                new_infect += list(np.extract(success, list(G.neighbors(node))))
        
        I.extend(new_infect)
        I = list(set(I))
    
    return(len(I))  

def sigma_mc(E, S, lam, nsims=100, mc_cores=7):

    # Repeat function nsims times
    pool = get_context("fork").Pool(mc_cores)
    out = pool.starmap(sigma, zip(repeat(E), repeat(S), repeat(lam), range(nsims)))
    pool.close()
                       
    out = list(out)
    
    return(statistics.mean(out), statistics.stdev(out)/math.sqrt(len(out)) )


# NOTE: we can use CELF because temporal SI model IS submodular according to Erkol et al. (2022)

def greedy(E, k, lam, nsims=100, mc_cores=7):
    
    S = []     # optimal seed set
    times = [] # computation time for each node
    
    G = nx.from_pandas_edgelist(E, 'v1', 'v2') 
    start = time.time()
    
    spread = [sigma_mc(E, [node], lam, nsims, mc_cores)[0] for node in G.nodes()]
    
    # Sort in decreasing order of spread
    Q = sorted( zip(G.nodes(),spread), key=lambda x: x[1],reverse=True)

    # Add node with largest spread to seed set
    cum_spread = Q[0][1]
    S.append(Q[0][0])
    times.append(time.time()-start)
    
    LOOKUPS = [len(G.nodes())]
    
    # Remove from consideration
    Q = Q[1:]
    
    # Use CELF procedure to get k-1 remaining seed nodes

    for _ in range(k-1):
        
        check = False
        checked = []
        node_lookup = 0
        
        while not check:
            
            node_lookup += 1
        
            # Calculate new spread of top node
            cur_node = Q[0][0]
            
            # If we have already checked the current node, then it should be added to seed set
            if cur_node in checked:
                check = True
            else:
        
                Q[0] = (cur_node, sigma_mc(E, S + [cur_node], lam, nsims, mc_cores)[0] - cum_spread)
        
                # Re-sort list and see if top node remained unchanged
                Q = sorted(Q, key = lambda x: x[1], reverse = True)
        
                check = (Q[0][0] == cur_node)
                checked.append(cur_node)
        
        # Add the next node
        cum_spread += Q[0][1]
        S.append(Q[0][0])
        times.append(time.time()-start)
        LOOKUPS.append(node_lookup)
        
        # Remove from consideration
        Q = Q[1:]
        
    
    return S, cum_spread, times, LOOKUPS

def dyndeg(E, k, lam):
    #Initialize empty seed set
    S = []
    # Possible nodes to consider
    V = list(set(E["v1"]) or set(E["v2"]))
    n = len(V)

    ######## Step 1: Compute dynamic degrees

    # Initialize dynamic degree
    DD = [0]*n
    td = [0]*n
    # Temporal snapshots
    T = pd.unique(E["t"])

    # Create array where i th entry is the neighbors of nodes i at any time
    nodes_total = [[] for Null in range(n)]
    # Create array where i th entry is the neighbors of node i at time t-1
    nodes_prev = [[] for Null in range(n)]

    for v in range(n):
        # Find neighbors of node v at t=0 and probability of being neighbor (probability of an edge)
        nodes1        = list(E[(E["v1"]==V[v]) & (E["t"]==T[0])]["v2"])
        nodes2        = list(E[(E["v2"]==V[v]) & (E["t"]==T[0])]["v1"])
        nodes = nodes1 + nodes2

        if len(nodes)>0:
            nodes_prev[v] = list(set(nodes))
            nodes_total[v] = list(set(nodes))
    # Compute dynamic degree
    for t in T[1:len(T)]:
        # Create array where i th entry is the neighbors of node i at time t
        nodes_curr = [[] for Null in range(n)]
        for v in range(n):
            # Find neighbors at current time
            nodes1        = list(E[(E["v1"]==V[v]) & (E["t"]==t)]["v2"])
            nodes2        = list(E[(E["v2"]==V[v]) & (E["t"]==t)]["v1"])
            nodes = nodes1 + nodes2

            if len(nodes)>0:
                nodes_curr[v] = list(set(nodes))
                nodes_total[v] = list(set(nodes_total[v] + nodes_curr[v]))

            # Formula for dynamic degree from page 6 of Murata and Koga (2018)
            if len(nodes_prev[v]) > 0 or len(nodes_curr[v]) > 0:
                DD[v] += len(nodes_curr[v]) * (len(set(nodes_prev[v]) - set(nodes_curr[v]))) / (len(set(nodes_prev[v] + nodes_curr[v])))
        nodes_prev = nodes_curr


    ######## Step 2: Add node to seed with largest dynamic degrees and update values
    dd = DD
    for i in range(k):
        idx = dd.index(max(dd)) # Find max dd
        v = V[idx]
        S.append(v)             # and add it to seed set

        # Find which nodes were neighbors with v and update dd
        for u in range(n):
            if v in nodes_total[u]:
                td[u] += 1
                dd[u] = DD[u] - 2*td[u] - (DD[u]-td[u])*td[u]*lam

        # Remove node v from future consideration
        nodes_total[idx] = []
        dd[idx] = -100




    return(S)


def find_n(E0, S):
    S = [ int(x) for x in S ]
    return set(S + list(pd.unique(E0[E0["v1"].isin(S)]["v2"])))


def jac(L1, L2):
    L1 = set(L1)
    L2 = set(L2)
    return len(L1.intersection(L2)) / len(L1.union(L2))

def jac_kernel(neighs):
    N = len(neighs)
    Sigma = np.zeros((N,N))
    Sigma[0,0] = 1.01
    for i in range(1, N):
        for j in range(i):
            Sigma[i,j] = jac(neighs[i], neighs[j])
            Sigma[j,i] = Sigma[i,j]
        Sigma[i,i] = 1.01
        
    return Sigma


def hamming(x, y, kk):
    return 1 - np.sum(x!=y)/(2*kk)


def hamming_kernel(X, kk):
    N, C = X.shape
    Sigma = np.zeros((N,N))
    Sigma[0,0] = 1.01
    for i in range(1,N):
        for j in range(i):
            Sigma[i,j] = hamming(X[i,:], X[j,:], kk)
            Sigma[j,i] = Sigma[i,j]
        Sigma[i,i] = 1.01

    return Sigma


def bayes_post(y, X, Sigma, mc_burn=1000, mc_iters=5000):
    N = X.shape[0]
    n = X.shape[1]

    Sigmainv = np.linalg.inv(Sigma)
    Sigma_sum = np.ones(N)@Sigmainv@np.ones(N)
    Sigmay = np.ones(N)@Sigmainv@y
    
   # Initialize MCMC
    beta0 = 0
    sigma2 = 1 / np.random.gamma(1, 1/1)  

    beta0_post = np.zeros(mc_iters)
    sigma2_post = np.zeros(mc_iters)
    # GO! 
    for i in range(mc_burn+mc_iters):
        # Sample from beta0
        vvv = 1/(Sigma_sum/sigma2+1/100)
        mmm = Sigmay/sigma2
        beta0 = np.random.normal(mmm, math.sqrt(vvv),1)
        
        # Sample from sigma2
        sigma2 = 1/np.random.gamma(N/2, 1/(np.transpose(y)@Sigmainv@y/2))
                
        if i >= mc_burn:
            beta0_post[i-mc_burn]  = beta0
            sigma2_post[i-mc_burn] = sigma2
        
    return np.median(beta0_post), np.median(sigma2_post)

def mean_cov_func(Sstar, x, y, kk, beta0, sigma2, Sigma, kernel, neighs):
    NN = len(y)
    n = x.shape[1]
    E0 = E[E["t"]==0][["v1", "v2"]]
    Vinv = np.linalg.inv(Sigma)

    new_cov = np.zeros(NN)
    if kernel=="J":
        neighs_new = list(find_n(E0, Sstar)) 
        for i in range(NN):
            new_cov[i]  = jac(neighs[i], neighs_new)
            
    elif kernel=="H":
        xstar = np.zeros(n)
        xstar[Sstar] = 1
        for i in range(NN):
            new_cov[i]  = hamming(x[i, :], xstar, kk)   
        
    mu = beta0 + new_cov@Vinv@(y-beta0)
    sd = math.sqrt(sigma2*(1 - new_cov @ Vinv @ new_cov))

    return mu, sd



def EI(Sstar, x, y, kk, ystar, beta0, sigma2, Sigma, kernel, neighs):

    mu, sd = mean_cov_func(Sstar, x, y, kk, beta0, sigma2, Sigma, kernel, neighs)
    
    Delta = mu - ystar

    EI = max(Delta, 0) + sd * norm.pdf(Delta/sd) - abs(Delta)*norm.cdf(Delta/sd)
    
    return EI * (1 - math.sqrt(sigma2)/math.sqrt(sd*sd + sigma2))


        
def bayesopt(E, kk=1, kern="J", lam=0.01, N0=20, B=5, obj_sims=1000, mc_burn=1000, mc_iters=5000, mc_cores=7):

    start = time.time()
    
    # Sample points to evaluate objective function at.
    N = N0
    nsims = obj_sims


    # Compute degrees and sample proportionally
    G = nx.from_pandas_edgelist(E, 'v1', 'v2')
    n = len(G.nodes) # number of nodes
    x = np.zeros((N,n))
    y = np.zeros(N)
    
    degs = np.array(list(dict(G.degree()).values()))
    degs = degs / sum(degs)
    
    S_samp = np.zeros((N,kk))
    for i in range(N):
        samp = list(np.random.choice(a=n, size=kk, replace=False, p=degs))
        S_samp[i,:] = samp
        x[i, samp] = 1

        # Repeat until we get a new seed set.
        while list(x[i,:]) in x[:i, :].tolist():
            x[i, samp] = 0
            samp = list(np.random.choice(a=n, size=kk, replace=False, p=degs))
            S_samp[i,:] = samp
            x[i, samp] = 1
        
        
    out = [sigma_mc(E, list(S_samp[i,:]), lam, nsims, mc_cores)[0] for i in range(N)]    
    
    y = np.asarray(out)
    meany = statistics.mean(y)
    y = y - meany
    
    
    # Construct covariance matrix
    neighs = []
    if kern == "J":
        E0 = E[E["t"]==0][["v1", "v2"]]
        # Find neighbors of initial seed set
        neighs = [list(find_n(E0, S_samp[i])) for i in range(N)]

        # Compute Jaccard coefficient between all neighbor sets as measure of similarity
        Sigma = jac_kernel(neighs)

    elif kern=="H":
        Sigma = hamming_kernel(x, kk)

    # Greedy algorithm to find optimal node set based on EI
    def greedy2(kk, x, y, ystar, beta0, sigma2, Sigma, kernel, neighs):
    
        # All nodes
        nodes = set([x for x in range(n)])
        
        # Initialization
        s_cur = list(np.random.choice(a=n, size=kk, replace=False, p=degs))
    
        EI_cur = EI(s_cur, x, y, kk, ystar, beta0, sigma2, Sigma, kernel, neighs)
        ind = True

        # Candidates we have already tried
        x_list = x.tolist()
        
        while ind:
            ind = False
        
            # Randomize order
            random.shuffle(s_cur)
            
            for i in range(kk):
                # Swap ith element with all other nodes and keep largest
                for node in (nodes - set(s_cur)):
                    s_try = s_cur.copy()
                    s_try[i] = node

                    x_try = np.zeros(n)
                    x_try[s_try] = 1

                    # Only try if we haven't already considered
                    if list(x_try) not in x_list:
                        EI_try = EI(s_try, x, y, kk, ystar, beta0, sigma2, Sigma, kernel, neighs)
                    
                        if  EI_try > EI_cur:
                            EI_cur = EI_try.copy()
                            s_cur[i] = node
                            ind = True
        
        
        return s_cur

    # While still having iterations remaining
    for b in range(B):
        beta0, sigma2 = bayes_post(y=y, X=x, Sigma=Sigma, mc_burn=mc_burn, mc_iters=mc_iters)

        # Replace ystar with maximum of mean function evaluated at observed locations
        ystar = np.max([mean_cov_func([j for j in range(n) if x[i,j]==1], x, y, kk, beta0, sigma2, Sigma, kern, neighs)[0] for i in range(N0+b)])
        
        S_opt = greedy2(kk, x, y, ystar, beta0, sigma2, Sigma, kern, neighs)
        
        x_opt = np.zeros(n)
        x_opt[S_opt] = 1

        # Append observations to exisiting values
        y = np.append(y, sigma_mc(E, S_opt, lam, nsims, mc_cores)[0] - meany) 
        x = np.append(x, [x_opt], axis=0)
 

        # Update covariance matrix
        if kern=="J":
            neighs.append(list(find_n(E0, S_opt)))
            new_cov = np.zeros((1,N+b))
            for i in range(N + b):
                new_cov[0,i]  = jac(neighs[i], neighs[N+b])
        
        elif kern=="H":
            new_cov = np.zeros((1,N+b))
            for i in range(N + b):
                new_cov[0,i]  = hamming(x[i, :], x[N+b, :], kk)  
            
        Sigma = np.append(Sigma, new_cov, 0)
        new_cov = np.array(np.append(new_cov,1))
        new_cov = new_cov.reshape(-1, 1)
        Sigma = np.concatenate((Sigma, new_cov), axis=1)
        Sigma[N+b, N+b] = 1.01       


    # Select largest value
    yvec = [mean_cov_func([j for j in range(n) if x[i,j]==1], x, y, kk, beta0, sigma2, Sigma, kern, neighs)[0] for i in range(N0+b)]
    y_opt = np.max(yvec) 

    S_opt = list(np.where(x[np.argmax(yvec),:]>0)[0])

    end = time.time()
        
    return S_opt, y_opt, (end-start)
