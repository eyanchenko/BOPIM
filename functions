# Load libraries
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import random
import math
import statistics
import time
import warnings
warnings.filterwarnings('ignore')

from scipy.optimize import minimize
from multiprocessing import Pool
import networkx as nx
from multiprocessing import get_context
from itertools import repeat
from itertools import chain
from multiprocessing import Pool
from multiprocessing import get_context
from igraph import Graph
from scipy.stats import norm
import scipy

try:
    from tqdm import tqdm
except ImportError:
    def tqdm(iterable):
        return iterable


# Function to pre-process (bin) times
def pre_process_times(E, grps = 10):
    out = E.copy()
    if type(grps)!=int:
        out["t"] = pd.cut(out["t"], bins=grps, labels=list(range(len(grps)-1)))
    else:
        out["t"] = pd.cut(out["t"], bins=grps, labels=list(range(grps)))

    out = out.drop_duplicates()
    out = out.reset_index(drop=True)

    # Convert node numbers from 0 to n
    G = nx.from_pandas_edgelist(E, 'v1', 'v2') 
    orig_nodes = list(G.nodes())
    node_dict = {orig_nodes[i]: i for i in range(len(orig_nodes))}
    out = pd.concat([out[["v1","v2"]].replace(to_replace=node_dict), out["t"]], axis=1)
    return out


# Function to simulate SI process (new idea)
# E:     input graph (as edge list)
# S:     seed nodes
# lam:   susceptibility parameter
def sigma(E, S, lam, xxx=0):
    # I will keep track of infected nodes
    I = list(S.copy())
    
    # Unique time snapshots of the network
    T = pd.unique(E["t"])
    
    for t in T:
    
        # Create graph
        G = nx.from_pandas_edgelist(E[E["t"]==t], 'v1', 'v2')
        new_infect = []
        # Loop over infected nodes
        for node in I:
            np.random.seed()
            if node in G.nodes:
                success = np.random.uniform(0,1,len(list(G.neighbors(node)))) < lam
                new_infect += list(np.extract(success, list(G.neighbors(node))))
        
        I.extend(new_infect)
        I = list(set(I))
    
    return(len(I))  

def sigma_mc(E, S, lam, nsims=100, mc_cores=7):

    # Repeat function nsims times
    pool = get_context("fork").Pool(mc_cores)
    out = pool.starmap(sigma, zip(repeat(E), repeat(S), repeat(lam), range(nsims)))
    pool.close()
                       
    out = list(out)
    
    return(statistics.mean(out), statistics.stdev(out)/math.sqrt(len(out)) )


# NOTE: we can use CELF because temporal SI model IS submodular according to Erkol et al. (2022)

def greedy(E, k, lam, nsims=100, mc_cores=7):
    
    S = []     # optimal seed set
    times = [] # computation time for each node
    
    G = nx.from_pandas_edgelist(E, 'v1', 'v2') 
    start = time.time()
    
    spread = [sigma_mc(E, [node], lam, nsims, mc_cores)[0] for node in G.nodes()]
    
    # Sort in decreasing order of spread
    Q = sorted( zip(G.nodes(),spread), key=lambda x: x[1],reverse=True)

    # Add node with largest spread to seed set
    cum_spread = Q[0][1]
    S.append(Q[0][0])
    times.append(time.time()-start)
    
    LOOKUPS = [len(G.nodes())]
    
    # Remove from consideration
    Q = Q[1:]
    
    # Use CELF procedure to get k-1 remaining seed nodes

    for _ in range(k-1):
        
        check = False
        checked = []
        node_lookup = 0
        
        while not check:
            
            node_lookup += 1
        
            # Calculate new spread of top node
            cur_node = Q[0][0]
            
            # If we have already checked the current node, then it should be added to seed set
            if cur_node in checked:
                check = True
            else:
        
                Q[0] = (cur_node, sigma_mc(E, S + [cur_node], lam, nsims, mc_cores)[0] - cum_spread)
        
                # Re-sort list and see if top node remained unchanged
                Q = sorted(Q, key = lambda x: x[1], reverse = True)
        
                check = (Q[0][0] == cur_node)
                checked.append(cur_node)
        
        # Add the next node
        cum_spread += Q[0][1]
        S.append(Q[0][0])
        times.append(time.time()-start)
        LOOKUPS.append(node_lookup)
        
        # Remove from consideration
        Q = Q[1:]
        
    
    return S, cum_spread, times, LOOKUPS

def dyndeg(E, k, lam):
    #Initialize empty seed set
    S = []
    # Possible nodes to consider
    V = list(set(E["v1"]) or set(E["v2"]))
    n = len(V)

    ######## Step 1: Compute dynamic degrees

    # Initialize dynamic degree
    DD = [0]*n
    td = [0]*n
    # Temporal snapshots
    T = pd.unique(E["t"])

    # Create array where i th entry is the neighbors of nodes i at any time
    nodes_total = [[] for Null in range(n)]
    # Create array where i th entry is the neighbors of node i at time t-1
    nodes_prev = [[] for Null in range(n)]

    for v in range(n):
        # Find neighbors of node v at t=0 and probability of being neighbor (probability of an edge)
        nodes1        = list(E[(E["v1"]==V[v]) & (E["t"]==T[0])]["v2"])
        nodes2        = list(E[(E["v2"]==V[v]) & (E["t"]==T[0])]["v1"])
        nodes = nodes1 + nodes2

        if len(nodes)>0:
            nodes_prev[v] = list(set(nodes))
            nodes_total[v] = list(set(nodes))
    # Compute dynamic degree
    for t in T[1:len(T)]:
        # Create array where i th entry is the neighbors of node i at time t
        nodes_curr = [[] for Null in range(n)]
        for v in range(n):
            # Find neighbors at current time
            nodes1        = list(E[(E["v1"]==V[v]) & (E["t"]==t)]["v2"])
            nodes2        = list(E[(E["v2"]==V[v]) & (E["t"]==t)]["v1"])
            nodes = nodes1 + nodes2

            if len(nodes)>0:
                nodes_curr[v] = list(set(nodes))
                nodes_total[v] = list(set(nodes_total[v] + nodes_curr[v]))

            # Formula for dynamic degree from page 6 of Murata and Koga (2018)
            if len(nodes_prev[v]) > 0 or len(nodes_curr[v]) > 0:
                DD[v] += len(nodes_curr[v]) * (len(set(nodes_prev[v]) - set(nodes_curr[v]))) / (len(set(nodes_prev[v] + nodes_curr[v])))
        nodes_prev = nodes_curr


    ######## Step 2: Add node to seed with largest dynamic degrees and update values
    dd = DD
    for i in range(k):
        idx = dd.index(max(dd)) # Find max dd
        v = V[idx]
        S.append(v)             # and add it to seed set

        # Find which nodes were neighbors with v and update dd
        for u in range(n):
            if v in nodes_total[u]:
                td[u] += 1
                dd[u] = DD[u] - 2*td[u] - (DD[u]-td[u])*td[u]*lam

        # Remove node v from future consideration
        nodes_total[idx] = []
        dd[idx] = -100




    return(S)


def find_n(E0, S):
    S = [ int(x) for x in S ]
    return set(S + list(pd.unique(E0[E0["v1"].isin(S)]["v2"])))


def jac(L1, L2):
    L1 = set(L1)
    L2 = set(L2)
    return len(L1.intersection(L2)) / len(L1.union(L2))

def horseshoe_cov(y, X, Sigma, mc_burn=1000, mc_iters=5000, gamma=1):
    N = X.shape[0]
    n = X.shape[1]
    
    loggamma = math.log(gamma)
    
    Sigmainv = np.linalg.inv(math.exp(loggamma)*Sigma + np.identity(N))
    
    # Initialize MCMC
    CHOL = np.linalg.cholesky(np.transpose(X) @ Sigmainv @ X + np.identity(n))
    alpha = CHOL @ np.random.normal(0, 1, n)
    sigma2 = 1 / np.random.gamma(1, 1/1)
    beta2 = np.zeros(n)
    for i in range(n):   
        beta2[i] = 1 / np.random.gamma(1, 1/1)
    
    tau2 = 1 / np.random.gamma(1, 1/1)
    nu = np.zeros(n)
    for i in range(n):   
        nu[i] = 1 / np.random.gamma(1, 1/1) 
    xi = 1 / np.random.gamma(1, 1/1)  
    
    alpha_post  = np.zeros((mc_iters,n))
    sigma2_post = np.zeros(mc_iters)
    
    # GO! 
    for i in range(mc_burn+mc_iters):
        # Sample from alpha

        Sinv = np.diag(1/(tau2*beta2))
        
        L = np.linalg.cholesky(np.transpose(X) @ Sigmainv @ X + Sinv)
        Linv = scipy.linalg.solve_triangular(L, np.identity(n))
        alpha = np.transpose(Linv) @ Linv @ np.transpose(X) @ Sigmainv @ y + math.sqrt(sigma2)*Linv @ np.random.normal(0, 1, n)

        XA = X@alpha
        
        # Sample from sigma2
        sigma2 = 1/np.random.gamma((N+n)/2, 1/(np.transpose(y-XA)@Sigmainv@(y-XA)/2 + sum(alpha**2/(2*tau2*beta2))))

        # Sample from beta2 and nu
        for k in range(n):
            beta2[k] = 1/np.random.gamma(1, 1/(1/nu[k] + alpha[k]**2/(2*tau2*sigma2)))
            nu[k] = 1/np.random.gamma(1, 1/(1+1/beta2[k]))

        # Sample from tau2
        tau2 = 1/np.random.gamma((n+1)/2, 1/(1/xi + 1/(2*sigma2)*sum(alpha**2/beta2)))

        # Sample from xi
        xi = 1/np.random.gamma(1, 1/(1+1/tau2))

        
        # Fix gamma at MLE     
                                                               
        if i >= mc_burn:
            alpha_post[i-mc_burn, :] = alpha
            sigma2_post[i-mc_burn] = sigma2


        
    return  np.median(alpha_post, axis=0), np.median(sigma2_post)


def mean_cov_func(Sstar, x, y, alpha, sigma2, gamma, Sigma, neighs):
    NN = len(y)
    E0 = E[E["t"]==0][["v1", "v2"]]
    Vinv = np.linalg.inv(gamma*sigma2*Sigma + sigma2*np.identity(NN))
    
    neighs_new = list(find_n(E0, Sstar))
    new_cov = np.zeros(NN)
    for i in range(NN):
        new_cov[i]  = gamma*sigma2*jac(neighs[i], neighs_new)
        
    mu = sum(alpha[Sstar])  + new_cov@Vinv@(y-x@alpha)
    sd = math.sqrt(sigma2*(gamma+1) - new_cov @ Vinv @ new_cov)

    return mu, sd


def EI(Sstar, x, y, ystar, alpha, sigma2, gamma, Sigma, neighs):

    mu, sd = mean_cov_func(Sstar, x, y, alpha, sigma2, gamma, Sigma, neighs)
    
    Delta = mu - ystar

    return max(Delta, 0) + sd * norm.pdf(Delta/sd) - abs(Delta)*norm.cdf(Delta/sd)


def bayesopt_new(E, kk=1, lam=0.01, N0=20, B=5, obj_sims=100, mc_burn=1000, mc_iters=5000, mc_cores=10):

    start = time.time()
    
    # Sample points to evaluate objective function at.
    N = N0
    nsims = obj_sims
    
    # Compute degrees and sample proportionally
    G = nx.from_pandas_edgelist(E, 'v1', 'v2')
    n = len(G.nodes) # number of nodes
    x = np.zeros((N,n))
    y = np.zeros(N)
    
    degs = np.array(list(dict(G.degree()).values()))
    degs = degs / sum(degs)
    
    S_samp = np.zeros((N,kk))
    for i in range(N):
        samp = list(np.random.choice(a=n, size=kk, replace=False, p=degs))
        S_samp[i,:] = samp
        x[i, samp] = 1
        
        
    out = [sigma_mc(E, list(S_samp[i,:]), lam, nsims, mc_cores)[0] for i in range(N)]    
    
    y = np.asarray(out)
    meany = statistics.mean(y)
    y = y - meany
    ystar = max(y)

    cur_max = []
    cur_max.append(ystar + meany)
    
    # Construct covariance matrix
    neighs = []
    E0 = E[E["t"]==0][["v1", "v2"]]
    # Find neighbors of initial seed set
    neighs = [list(find_n(E0, S_samp[i])) for i in range(N)]

    # Compute Jaccard coefficient between all neighbor sets as measure of similarity
    Sigma = np.zeros((N,N))
    Sigma[0,0] = 1.01
    for i in range(1, N):
        for j in range(i):
            Sigma[i,j] = jac(neighs[i], neighs[j])
            Sigma[j,i] = Sigma[i,j]
        Sigma[i,i] = 1.01

    # Greedy algorithm to find optimal node set based on EI
    def greedy2(kk, x, y, ystar, alpha, sigma2, gamma, Sigma, neighs):
    
        # All nodes
        nodes = set([x for x in range(n)])
        
        # Initialization
        s_cur = list(np.random.choice(a=n, size=kk, replace=False, p=degs))
    
        EI_cur = EI(s_cur, x, y, ystar, alpha, sigma2, gamma, Sigma, neighs)
        ind = True

        # Candidates we have already tried
        x_list = x.tolist()
        
        while ind:
            ind = False
        
            # Randomize order
            random.shuffle(s_cur)
            
            for i in range(kk):
                # Swap ith element with all other nodes and keep largest
                for node in (nodes - set(s_cur)):
                    s_try = s_cur.copy()
                    s_try[i] = node

                    x_try = np.zeros(n)
                    x_try[s_try] = 1

                    # Only try if we haven't already considered
                    if list(x_try) not in x_list:
                        EI_try = EI(s_try, x, y, ystar, alpha, sigma2, gamma, Sigma, neighs)
                    
                        if  EI_try > EI_cur:
                            EI_cur = EI_try.copy()
                            s_cur[i] = node
                            ind = True
        
        
        return s_cur

    # While still having iterations remaining
    for b in range(B):

        
        # Find MLE of gamma
        #def mle_func(loggamma):
        # sigma2 = statistics.variance(y)
        # return y @ np.linalg.inv(math.exp(loggamma)*Sigma + np.identity(len(y))) @ y/sigma2 + math.log(np.linalg.det(sigma2*(math.exp(loggamma)*Sigma + np.identity(len(y)))))
    
        # gamma_mle = math.exp(float(minimize(mle_func, math.log(1), method='nelder-mead').x))

        gamma_mle = 1
        alpha, sigma2 = horseshoe_cov(y=y, X=x, Sigma=Sigma, mc_burn=mc_burn, mc_iters=mc_iters, gamma=gamma_mle)



        S_opt = greedy2(kk, x, y, ystar, alpha, sigma2, gamma_mle, Sigma, neighs)
        
        x_opt = np.zeros(n)
        x_opt[S_opt] = 1

        
        y = np.append(y, sigma_mc(E, S_opt, lam, nsims, mc_cores)[0] - meany) 
        x = np.append(x, [x_opt], axis=0)
        ystar = max(y)

        cur_max.append(ystar + meany)

 

        # Update covariance matrix
        neighs.append(list(find_n(E0, S_opt)))
        new_cov = np.zeros((1,N+b))
        for i in range(N + b):
            new_cov[0,i]  = jac(neighs[i], neighs[N+b])
                
        Sigma = np.append(Sigma, new_cov, 0)
        new_cov = np.array(np.append(new_cov,1))
        new_cov = new_cov.reshape(-1, 1)
        Sigma = np.concatenate((Sigma, new_cov), axis=1)
        Sigma[N+b, N+b] = 1.01

        

    # Select largest value
    y_opt = (max(y) + meany)
    S_opt = list(np.where(x[np.argmax(y),:]>0)[0])

    end = time.time()
        
    return S_opt, y_opt, (end-start)

